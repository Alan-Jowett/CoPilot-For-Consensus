# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

"""Generated typed configuration for llm_backend adapter.

DO NOT EDIT THIS FILE MANUALLY.
This file is auto-generated from JSON schemas by scripts/generate_typed_configs.py.
"""

from dataclasses import dataclass
from typing import Any, Dict, List, Literal, Optional, Union


@dataclass
class DriverConfig_LlmBackend_AzureOpenaiGpt:
    """Configuration for llm_backend adapter using azure_openai_gpt driver."""
    azure_openai_api_key: str
    """Azure OpenAI API key"""
    azure_openai_endpoint: str
    # Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com/)
    azure_openai_model: str
    # Model name (used when invoking Azure OpenAI deployment)
    azure_openai_api_version: Optional[str] = '2023-12-01'
    # Azure OpenAI API version
    azure_openai_deployment: Optional[str] = None
    # Azure OpenAI deployment name


@dataclass
class DriverConfig_LlmBackend_Llamacpp:
    """Configuration for llm_backend adapter using llamacpp driver."""
    llamacpp_endpoint: Optional[str] = 'http://llama-cpp:8081'
    """llama.cpp server endpoint URL"""
    llamacpp_model: Optional[str] = 'mistral'
    # llama.cpp model name
    llamacpp_timeout_seconds: Optional[int] = 300
    # Request timeout in seconds


@dataclass
class DriverConfig_LlmBackend_Local:
    """Configuration for llm_backend adapter using local driver."""
    local_llm_endpoint: Optional[str] = 'http://ollama:11434'
    """Local LLM endpoint URL (e.g., Ollama)"""
    local_llm_model: Optional[str] = 'mistral'
    # Local model name (e.g., mistral, llama2)
    local_llm_timeout_seconds: Optional[int] = 300
    # Request timeout in seconds


@dataclass
class DriverConfig_LlmBackend_Mock:
    """Configuration for llm_backend adapter using mock driver."""
    mock_latency_ms: Optional[int] = 100
    """Simulated latency in milliseconds"""


@dataclass
class DriverConfig_LlmBackend_Openai:
    """Configuration for llm_backend adapter using openai driver."""
    openai_api_key: str
    """OpenAI API key"""
    openai_model: str
    # OpenAI model name (e.g., gpt-4o, gpt-3.5-turbo)
    openai_base_url: Optional[str] = None
    # Optional custom OpenAI-compatible base URL


@dataclass
class AdapterConfig_LlmBackend:
    """Configuration for llm_backend adapter."""
    llm_backend_type: Literal["azure_openai_gpt", "llamacpp", "local", "mock", "openai"]
    driver: Union[
        DriverConfig_LlmBackend_AzureOpenaiGpt,
        DriverConfig_LlmBackend_Llamacpp,
        DriverConfig_LlmBackend_Local,
        DriverConfig_LlmBackend_Mock,
        DriverConfig_LlmBackend_Openai
    ]
