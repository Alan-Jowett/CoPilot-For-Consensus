# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

"""Generated typed configuration for summarization service.

DO NOT EDIT THIS FILE MANUALLY.
This file is auto-generated from JSON schemas by scripts/generate_typed_configs.py.
"""

from dataclasses import dataclass
from typing import Any, Literal


@dataclass
class DriverConfig_DocumentStore_AzureCosmosdb:
    """Configuration for document_store adapter using azure_cosmosdb driver."""
    container: str | None = 'documents'
    """Cosmos DB container name"""
    database: str | None = 'copilot'
    # Cosmos DB database name
    endpoint: str | None = None
    # Cosmos DB endpoint URL (e.g., https://myaccount.documents.azure.com:443/)
    key: str | None = None
    # Cosmos DB account key (optional if using managed identity)
    partition_key: str | None = '/collection'
    # Cosmos DB partition key path


@dataclass
class DriverConfig_DocumentStore_Inmemory:
    """Configuration for document_store adapter using inmemory driver."""
    pass


@dataclass
class DriverConfig_DocumentStore_Mongodb:
    """Configuration for document_store adapter using mongodb driver."""
    database: str | None = 'copilot'
    """MongoDB database name"""
    host: str | None = 'documentdb'
    # MongoDB hostname
    password: str | None = None
    # MongoDB password
    port: int | None = 27017
    # MongoDB port
    username: str | None = None
    # MongoDB username


@dataclass
class AdapterConfig_DocumentStore:
    """Configuration for document_store adapter."""
    doc_store_type: Literal["azure_cosmosdb", "inmemory", "mongodb"]
    driver: (
        DriverConfig_DocumentStore_AzureCosmosdb |
        DriverConfig_DocumentStore_Inmemory |
        DriverConfig_DocumentStore_Mongodb
    )


@dataclass
class DriverConfig_EmbeddingBackend_AzureOpenai:
    """Configuration for embedding_backend adapter using azure_openai driver."""
    api_base: str | None = None
    """Azure OpenAI endpoint URL"""
    api_key: str | None = None
    # Azure OpenAI API key
    api_version: str | None = '2024-02-15-preview'
    # Azure OpenAI API version
    deployment_name: str | None = None
    # Azure OpenAI deployment name for embeddings
    model: str | None = None
    # Optional model name to map to deployment


@dataclass
class DriverConfig_EmbeddingBackend_Huggingface:
    """Configuration for embedding_backend adapter using huggingface driver."""
    cache_dir: str | None = None
    """Cache directory for model files"""
    device: str | None = None
    # Device to use (cpu, cuda, mps)
    max_length: int | None = 512
    # Maximum sequence length for tokenization
    model_name: str | None = None
    # HuggingFace model name (e.g., 'sentence-transformers/all-MiniLM-L6-v2')


@dataclass
class DriverConfig_EmbeddingBackend_Mock:
    """Configuration for embedding_backend adapter using mock driver."""
    dimension: int | None = 384
    """Embedding dimension for mock backend"""


@dataclass
class DriverConfig_EmbeddingBackend_Openai:
    """Configuration for embedding_backend adapter using openai driver."""
    api_key: str | None = None
    """OpenAI API key"""
    model: str | None = None
    # OpenAI embedding model name
    organization: str | None = None
    # Optional OpenAI organization id


@dataclass
class DriverConfig_EmbeddingBackend_Sentencetransformers:
    """Configuration for embedding_backend adapter using sentencetransformers driver."""
    cache_dir: str | None = None
    """Cache folder for model files"""
    device: str | None = 'cpu'
    # Device to use (cpu, cuda, mps)
    model_name: str | None = 'all-MiniLM-L6-v2'
    # SentenceTransformers model name to use


@dataclass
class AdapterConfig_EmbeddingBackend:
    """Configuration for embedding_backend adapter."""
    embedding_backend_type: Literal["azure_openai", "huggingface", "mock", "openai", "sentencetransformers"]
    driver: (
        DriverConfig_EmbeddingBackend_AzureOpenai |
        DriverConfig_EmbeddingBackend_Huggingface |
        DriverConfig_EmbeddingBackend_Mock |
        DriverConfig_EmbeddingBackend_Openai |
        DriverConfig_EmbeddingBackend_Sentencetransformers
    )


@dataclass
class DriverConfig_ErrorReporter_Console:
    """Configuration for error_reporter adapter using console driver."""
    logger_name: str | None = None
    """Logger name used by the console reporter (defaults to service logger)"""


@dataclass
class DriverConfig_ErrorReporter_Sentry:
    """Configuration for error_reporter adapter using sentry driver."""
    dsn: str | None = None
    """Sentry DSN for reporting errors"""
    environment: str | None = 'production'
    # Environment name to tag Sentry events


@dataclass
class DriverConfig_ErrorReporter_Silent:
    """Configuration for error_reporter adapter using silent driver."""
    pass


@dataclass
class AdapterConfig_ErrorReporter:
    """Configuration for error_reporter adapter."""
    error_reporter_type: Literal["console", "sentry", "silent"]
    driver: DriverConfig_ErrorReporter_Console | DriverConfig_ErrorReporter_Sentry | DriverConfig_ErrorReporter_Silent


@dataclass
class DriverConfig_LlmBackend_AzureOpenaiGpt:
    """Configuration for llm_backend adapter using azure_openai_gpt driver."""
    azure_openai_api_key: str
    """Azure OpenAI API key"""
    azure_openai_api_version: str | None = '2023-12-01'
    # Azure OpenAI API version
    azure_openai_deployment: str | None = None
    # Azure OpenAI deployment name
    azure_openai_endpoint: str
    # Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com/)
    azure_openai_model: str
    # Model name (used when invoking Azure OpenAI deployment)


@dataclass
class DriverConfig_LlmBackend_Llamacpp:
    """Configuration for llm_backend adapter using llamacpp driver."""
    llamacpp_endpoint: str | None = 'http://llama-cpp:8081'
    """llama.cpp server endpoint URL"""
    llamacpp_model: str | None = 'mistral'
    # llama.cpp model name
    llamacpp_timeout_seconds: int | None = 300
    # Request timeout in seconds


@dataclass
class DriverConfig_LlmBackend_Local:
    """Configuration for llm_backend adapter using local driver."""
    local_llm_endpoint: str | None = 'http://ollama:11434'
    """Local LLM endpoint URL (e.g., Ollama)"""
    local_llm_model: str | None = 'mistral'
    # Local model name (e.g., mistral, llama2)
    local_llm_timeout_seconds: int | None = 300
    # Request timeout in seconds


@dataclass
class DriverConfig_LlmBackend_Mock:
    """Configuration for llm_backend adapter using mock driver."""
    mock_latency_ms: int | None = 100
    """Simulated latency in milliseconds"""


@dataclass
class DriverConfig_LlmBackend_Openai:
    """Configuration for llm_backend adapter using openai driver."""
    openai_api_key: str
    """OpenAI API key"""
    openai_base_url: str | None = None
    # Optional custom OpenAI-compatible base URL
    openai_model: str
    # OpenAI model name (e.g., gpt-4o, gpt-3.5-turbo)


@dataclass
class AdapterConfig_LlmBackend:
    """Configuration for llm_backend adapter."""
    llm_backend_type: Literal["azure_openai_gpt", "llamacpp", "local", "mock", "openai"]
    driver: (
        DriverConfig_LlmBackend_AzureOpenaiGpt |
        DriverConfig_LlmBackend_Llamacpp |
        DriverConfig_LlmBackend_Local |
        DriverConfig_LlmBackend_Mock |
        DriverConfig_LlmBackend_Openai
    )


@dataclass
class DriverConfig_Logger_AzureMonitor:
    """Configuration for logger adapter using azure_monitor driver."""
    console_log: bool | None = False
    """Also log to console when using Azure Monitor"""
    instrumentation_key: str | None = None
    # Application Insights instrumentation key for Azure Monitor logging
    level: str | None = 'INFO'
    # Logging level
    name: str | None = 'copilot'
    # Logger name for identification


@dataclass
class DriverConfig_Logger_Silent:
    """Configuration for logger adapter using silent driver."""
    level: str | None = 'INFO'
    """Logging level for silent logger"""
    name: str | None = None
    # Logger name (optional)


@dataclass
class DriverConfig_Logger_Stdout:
    """Configuration for logger adapter using stdout driver."""
    level: str | None = 'INFO'
    """Logging level for stdout logger"""
    name: str | None = None
    # Logger name (optional)


@dataclass
class AdapterConfig_Logger:
    """Configuration for logger adapter."""
    logger_type: Literal["azure_monitor", "silent", "stdout"]
    driver: DriverConfig_Logger_AzureMonitor | DriverConfig_Logger_Silent | DriverConfig_Logger_Stdout


@dataclass
class DriverConfig_MessageBus_AzureServiceBus:
    """Configuration for message_bus adapter using azure_service_bus driver."""
    auto_complete: bool | None = False
    """Automatically complete messages after processing"""
    connection_string: str | None = None
    # Azure Service Bus connection string (used when not using managed identity)
    max_wait_time: int | None = 5
    # Maximum time to wait for messages in seconds
    queue_name: str | None = None
    # Azure Service Bus queue name (optional)
    servicebus_fully_qualified_namespace: str | None = None
    # Azure Service Bus fully qualified namespace
    servicebus_use_managed_identity: bool | None = False
    # Use managed identity for Service Bus access
    subscription_name: str | None = None
    # Azure Service Bus subscription name (optional, required if topic_name is set)
    topic_name: str | None = None
    # Azure Service Bus topic name (optional)


@dataclass
class DriverConfig_MessageBus_Noop:
    """Configuration for message_bus adapter using noop driver."""
    pass


@dataclass
class DriverConfig_MessageBus_Rabbitmq:
    """Configuration for message_bus adapter using rabbitmq driver."""
    auto_ack: bool | None = False
    """Automatically acknowledge messages after processing"""
    exchange: str | None = 'copilot.events'
    # RabbitMQ exchange name for publishing
    exchange_name: str | None = 'copilot.events'
    # RabbitMQ exchange name for subscribing
    exchange_type: str | None = 'topic'
    # RabbitMQ exchange type
    queue_durable: bool | None = True
    # Whether the queue survives broker restart
    queue_name: str | None = None
    # RabbitMQ queue name (optional for subscribers)
    rabbitmq_host: str | None = 'messagebus'
    # RabbitMQ hostname
    rabbitmq_password: str | None = None
    # RabbitMQ password
    rabbitmq_port: int | None = 5672
    # RabbitMQ port
    rabbitmq_username: str | None = None
    # RabbitMQ username


@dataclass
class AdapterConfig_MessageBus:
    """Configuration for message_bus adapter."""
    message_bus_type: Literal["azure_service_bus", "noop", "rabbitmq"]
    driver: DriverConfig_MessageBus_AzureServiceBus | DriverConfig_MessageBus_Noop | DriverConfig_MessageBus_Rabbitmq


@dataclass
class DriverConfig_Metrics_AzureMonitor:
    """Configuration for metrics adapter using azure_monitor driver."""
    azure_monitor_instrumentation_key: str | None = None
    """Application Insights instrumentation key"""
    connection_string: str | None = None
    # Azure Monitor connection string (e.g., InstrumentationKey=... or Connection string)
    export_interval_millis: int | None = 60000
    # Export interval in milliseconds
    namespace: str | None = 'copilot'
    # Namespace prefix for all metrics
    raise_on_error: bool | None = False
    # Whether to raise exceptions on metric errors


@dataclass
class DriverConfig_Metrics_Noop:
    """Configuration for metrics adapter using noop driver."""
    pass


@dataclass
class DriverConfig_Metrics_Prometheus:
    """Configuration for metrics adapter using prometheus driver."""
    namespace: str | None = 'copilot'
    """Namespace prefix for all metrics"""
    raise_on_error: bool | None = False
    # Whether to raise exceptions on metric errors
    registry: dict[str, Any] | None = None
    # Optional Prometheus registry instance


@dataclass
class DriverConfig_Metrics_Pushgateway:
    """Configuration for metrics adapter using pushgateway driver."""
    gateway: str
    """Pushgateway address (e.g., pushgateway:9091 or http://pushgateway:9091)"""
    grouping_key: dict[str, Any] | None = None
    # Optional grouping key dictionary for metric grouping
    job: str | None = None
    # Prometheus job name for this service (can be set programmatically)
    namespace: str | None = 'copilot'
    # Metric namespace prefix
    raise_on_error: bool | None = False
    # Whether to raise on metric collection errors


@dataclass
class AdapterConfig_Metrics:
    """Configuration for metrics adapter."""
    metrics_type: Literal["azure_monitor", "noop", "prometheus", "pushgateway"]
    driver: (
        DriverConfig_Metrics_AzureMonitor |
        DriverConfig_Metrics_Noop |
        DriverConfig_Metrics_Prometheus |
        DriverConfig_Metrics_Pushgateway
    )


@dataclass
class DriverConfig_SecretProvider_AzureKeyVault:
    """Configuration for secret_provider adapter using azure_key_vault driver."""
    client_id: str | None = None
    """Azure client ID (optional; managed identity may be used)"""
    client_secret: str | None = None
    # Azure client secret (optional; managed identity may be used)
    tenant_id: str | None = None
    # Azure tenant ID (optional; managed identity may be used)
    vault_name: str | None = None
    # Azure Key Vault name (optional when vault_url is provided)
    vault_url: str | None = None
    # Azure Key Vault URL (e.g., https://my-vault.vault.azure.net/)


@dataclass
class DriverConfig_SecretProvider_Local:
    """Configuration for secret_provider adapter using local driver."""
    base_path: str | None = '/run/secrets'
    """Base path for secret storage"""


@dataclass
class AdapterConfig_SecretProvider:
    """Configuration for secret_provider adapter."""
    secret_provider_type: Literal["azure_key_vault", "local"]
    driver: DriverConfig_SecretProvider_AzureKeyVault | DriverConfig_SecretProvider_Local


@dataclass
class DriverConfig_VectorStore_AzureAiSearch:
    """Configuration for vector_store adapter using azure_ai_search driver."""
    api_key: str | None = None
    """Azure AI Search API key (alternative to managed identity)"""
    endpoint: str | None = None
    # Azure AI Search endpoint URL
    index_name: str | None = 'embeddings'
    # Azure AI Search index name for embeddings
    use_managed_identity: bool | None = False
    # Use managed identity instead of API key
    vector_size: int | None = 384
    # Embedding vector dimension


@dataclass
class DriverConfig_VectorStore_Faiss:
    """Configuration for vector_store adapter using faiss driver."""
    dimension: int | None = 384
    """Embedding vector dimension"""
    index_type: str | None = 'flat'
    # FAISS index type (flat, ivf)
    persist_path: str | None = None
    # Optional path to persist the index


@dataclass
class DriverConfig_VectorStore_Inmemory:
    """Configuration for vector_store adapter using inmemory driver."""
    pass


@dataclass
class DriverConfig_VectorStore_Qdrant:
    """Configuration for vector_store adapter using qdrant driver."""
    api_key: str | None = None
    """Qdrant API key (optional)"""
    collection_name: str | None = 'embeddings'
    # Qdrant collection name for embeddings
    distance: str | None = 'cosine'
    # Distance metric (cosine, euclid)
    host: str | None = 'vectorstore'
    # Qdrant server hostname
    port: int | None = 6333
    # Qdrant server port
    upsert_batch_size: int | None = 100
    # Batch size for upsert operations
    vector_size: int | None = 384
    # Embedding vector dimension


@dataclass
class AdapterConfig_VectorStore:
    """Configuration for vector_store adapter."""
    vector_store_type: Literal["azure_ai_search", "faiss", "inmemory", "qdrant"]
    driver: (
        DriverConfig_VectorStore_AzureAiSearch |
        DriverConfig_VectorStore_Faiss |
        DriverConfig_VectorStore_Inmemory |
        DriverConfig_VectorStore_Qdrant
    )


@dataclass
class ServiceSettings_Summarization:
    """Service-specific settings for summarization."""
    auth_service_url: str | None = 'http://auth:8090'
    citation_count: int | None = 12
    http_port: int | None = 8000
    jwt_auth_enabled: bool | None = True
    max_retries: int | None = 3
    retry_delay_seconds: int | None = 5
    service_audience: str | None = 'copilot-for-consensus'
    top_k: int | None = 12


@dataclass
class ServiceConfig_Summarization:
    """Top-level configuration for summarization service."""
    service_settings: ServiceSettings_Summarization
    document_store: AdapterConfig_DocumentStore | None = None
    embedding_backend: AdapterConfig_EmbeddingBackend | None = None
    error_reporter: AdapterConfig_ErrorReporter | None = None
    llm_backend: AdapterConfig_LlmBackend | None = None
    logger: AdapterConfig_Logger | None = None
    message_bus: AdapterConfig_MessageBus | None = None
    metrics: AdapterConfig_Metrics | None = None
    secret_provider: AdapterConfig_SecretProvider | None = None
    vector_store: AdapterConfig_VectorStore | None = None
