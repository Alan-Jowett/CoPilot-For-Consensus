# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

"""Tests for LocalLLMSummarizer."""

from unittest.mock import Mock, patch

import pytest
import requests
from copilot_summarization.local_llm_summarizer import LocalLLMSummarizer
from copilot_summarization.models import Thread


class TestLocalLLMSummarizer:
    """Tests for LocalLLMSummarizer implementation."""

    def test_local_llm_summarizer_creation(self):
        """Test creating a local LLM summarizer."""
        summarizer = LocalLLMSummarizer()
        assert summarizer.model == "mistral"
        assert summarizer.base_url == "http://localhost:11434"
        assert summarizer.timeout == 120

    def test_local_llm_summarizer_custom_config(self):
        """Test creating a local LLM summarizer with custom config."""
        summarizer = LocalLLMSummarizer(
            model="llama2",
            base_url="http://custom:8080",
            timeout=60
        )
        assert summarizer.model == "llama2"
        assert summarizer.base_url == "http://custom:8080"
        assert summarizer.timeout == 60

    def test_local_llm_summarizer_invalid_timeout(self):
        """Test creating a local LLM summarizer with invalid timeout raises ValueError."""
        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LocalLLMSummarizer(timeout=0)

        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LocalLLMSummarizer(timeout=-1)

        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LocalLLMSummarizer(timeout="120")  # type: ignore

    @patch('copilot_summarization.local_llm_summarizer.requests.post')
    def test_local_llm_summarize_success(self, mock_post):
        """Test local LLM summarize returns real content from API."""
        # Mock successful API response
        mock_response = Mock()
        mock_response.json.return_value = {
            "response": "This is a summary of the discussion thread generated by Ollama."
        }
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response

        summarizer = LocalLLMSummarizer(model="mistral")

        thread = Thread(
            thread_id="test-thread-123",
            messages=["Message 1", "Message 2"]
        )

        summary = summarizer.summarize(thread)

        # Verify API was called
        mock_post.assert_called_once()
        call_args = mock_post.call_args
        assert call_args[0][0] == "http://localhost:11434/api/generate"
        assert call_args[1]["json"]["model"] == "mistral"
        assert call_args[1]["json"]["stream"] is False

        # Verify prompt template structure
        prompt = call_args[1]["json"]["prompt"]
        assert "Summarize the following discussion thread:" in prompt
        assert "Message 1:" in prompt
        assert "Message 2:" in prompt
        assert "Message 1" in prompt
        assert "Message 2" in prompt

        # Verify summary contains real content (not placeholder)
        assert summary.thread_id == "test-thread-123"
        assert summary.summary_markdown == "This is a summary of the discussion thread generated by Ollama."
        assert "Local LLM Summary Placeholder" not in summary.summary_markdown
        assert "scaffold implementation" not in summary.summary_markdown
        assert summary.llm_backend == "local"
        assert summary.llm_model == "mistral"
        assert summary.tokens_prompt > 0
        assert summary.tokens_completion > 0
        assert summary.latency_ms >= 0

    @patch('copilot_summarization.local_llm_summarizer.requests.post')
    def test_local_llm_summarize_empty_response(self, mock_post):
        """Test local LLM handles empty response gracefully."""
        # Mock empty API response
        mock_response = Mock()
        mock_response.json.return_value = {"response": ""}
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response

        summarizer = LocalLLMSummarizer(model="mistral")

        thread = Thread(
            thread_id="test-thread-456",
            messages=["Message 1"]
        )

        summary = summarizer.summarize(thread)

        # Should return fallback message
        assert "Unable to generate summary" in summary.summary_markdown
        assert summary.thread_id == "test-thread-456"

    @patch('copilot_summarization.local_llm_summarizer.requests.post')
    def test_local_llm_summarize_timeout(self, mock_post):
        """Test local LLM handles timeout errors."""
        mock_post.side_effect = requests.Timeout("Request timed out")

        summarizer = LocalLLMSummarizer(model="mistral")

        thread = Thread(
            thread_id="test-thread-789",
            messages=["Message 1"]
        )

        with pytest.raises(requests.Timeout):
            summarizer.summarize(thread)

    @patch('copilot_summarization.local_llm_summarizer.requests.post')
    def test_local_llm_summarize_connection_error(self, mock_post):
        """Test local LLM handles connection errors."""
        mock_post.side_effect = requests.ConnectionError("Failed to connect")

        summarizer = LocalLLMSummarizer(model="mistral")

        thread = Thread(
            thread_id="test-thread-999",
            messages=["Message 1"]
        )

        with pytest.raises(requests.ConnectionError):
            summarizer.summarize(thread)
