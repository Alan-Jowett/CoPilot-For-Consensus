# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

"""Tests for LlamaCppSummarizer."""

import pytest
import requests
from unittest.mock import patch, Mock
from copilot_summarization.llamacpp_summarizer import LlamaCppSummarizer
from copilot_summarization.models import Thread


class TestLlamaCppSummarizer:
    """Tests for LlamaCppSummarizer implementation."""
    
    def test_llamacpp_summarizer_creation(self):
        """Test creating a llama.cpp summarizer."""
        summarizer = LlamaCppSummarizer()
        assert summarizer.model == "mistral"
        assert summarizer.base_url == "http://localhost:8080"
        assert summarizer.timeout == 120
    
    def test_llamacpp_summarizer_custom_config(self):
        """Test creating a llama.cpp summarizer with custom config."""
        summarizer = LlamaCppSummarizer(
            model="mistral-7b-instruct-v0.2.Q4_K_M",
            base_url="http://llama-cpp:8080",
            timeout=60
        )
        assert summarizer.model == "mistral-7b-instruct-v0.2.Q4_K_M"
        assert summarizer.base_url == "http://llama-cpp:8080"
        assert summarizer.timeout == 60
    
    def test_llamacpp_summarizer_invalid_timeout(self):
        """Test creating a llama.cpp summarizer with invalid timeout raises ValueError."""
        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LlamaCppSummarizer(timeout=0)
        
        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LlamaCppSummarizer(timeout=-1)
        
        with pytest.raises(ValueError, match="timeout must be a positive integer"):
            LlamaCppSummarizer(timeout="120")  # type: ignore
    
    @patch('copilot_summarization.llamacpp_summarizer.requests.post')
    def test_llamacpp_summarize_success(self, mock_post):
        """Test llama.cpp summarize returns real content from API."""
        # Mock successful API response
        mock_response = Mock()
        mock_response.json.return_value = {
            "content": "This is a summary of the discussion thread generated by llama.cpp."
        }
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response
        
        summarizer = LlamaCppSummarizer(model="mistral-7b-instruct-v0.2.Q4_K_M")
        
        thread = Thread(
            thread_id="test-thread-123",
            messages=["Message 1", "Message 2"]
        )
        
        summary = summarizer.summarize(thread)
        
        # Verify API was called
        mock_post.assert_called_once()
        call_args = mock_post.call_args
        assert call_args[0][0] == "http://localhost:8080/completion"
        assert call_args[1]["json"]["prompt"]
        assert call_args[1]["json"]["n_predict"] == 512
        assert call_args[1]["json"]["temperature"] == 0.7
        assert "stop" in call_args[1]["json"]
        
        # Verify prompt template structure
        prompt = call_args[1]["json"]["prompt"]
        assert "Summarize the following discussion thread:" in prompt
        assert "Message 1:" in prompt
        assert "Message 2:" in prompt
        assert "Message 1" in prompt
        assert "Message 2" in prompt
        
        # Verify summary contains real content
        assert summary.thread_id == "test-thread-123"
        assert summary.summary_markdown == "This is a summary of the discussion thread generated by llama.cpp."
        assert summary.llm_backend == "llamacpp"
        assert summary.llm_model == "mistral-7b-instruct-v0.2.Q4_K_M"
        assert summary.tokens_prompt > 0
        assert summary.tokens_completion > 0
        assert summary.latency_ms >= 0
    
    @patch('copilot_summarization.llamacpp_summarizer.requests.post')
    def test_llamacpp_summarize_empty_response(self, mock_post):
        """Test llama.cpp handles empty response gracefully."""
        # Mock empty API response
        mock_response = Mock()
        mock_response.json.return_value = {"content": ""}
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response
        
        summarizer = LlamaCppSummarizer(model="mistral-7b-instruct-v0.2.Q4_K_M")
        
        thread = Thread(
            thread_id="test-thread-456",
            messages=["Message 1"]
        )
        
        summary = summarizer.summarize(thread)
        
        # Should return fallback message
        assert "Unable to generate summary" in summary.summary_markdown
        assert summary.thread_id == "test-thread-456"
        assert summary.tokens_completion == 0
    
    @patch('copilot_summarization.llamacpp_summarizer.requests.post')
    def test_llamacpp_summarize_timeout(self, mock_post):
        """Test llama.cpp handles timeout errors."""
        mock_post.side_effect = requests.Timeout("Request timed out")
        
        summarizer = LlamaCppSummarizer(model="mistral-7b-instruct-v0.2.Q4_K_M")
        
        thread = Thread(
            thread_id="test-thread-789",
            messages=["Message 1"]
        )
        
        with pytest.raises(requests.Timeout):
            summarizer.summarize(thread)
    
    @patch('copilot_summarization.llamacpp_summarizer.requests.post')
    def test_llamacpp_summarize_connection_error(self, mock_post):
        """Test llama.cpp handles connection errors."""
        mock_post.side_effect = requests.ConnectionError("Failed to connect")
        
        summarizer = LlamaCppSummarizer(model="mistral-7b-instruct-v0.2.Q4_K_M")
        
        thread = Thread(
            thread_id="test-thread-999",
            messages=["Message 1"]
        )
        
        with pytest.raises(requests.ConnectionError):
            summarizer.summarize(thread)
    
    @patch('copilot_summarization.llamacpp_summarizer.requests.post')
    def test_llamacpp_summarize_http_error(self, mock_post):
        """Test llama.cpp handles HTTP errors."""
        mock_response = Mock()
        mock_response.raise_for_status.side_effect = requests.HTTPError("500 Server Error")
        mock_post.return_value = mock_response
        
        summarizer = LlamaCppSummarizer(model="mistral-7b-instruct-v0.2.Q4_K_M")
        
        thread = Thread(
            thread_id="test-thread-500",
            messages=["Message 1"]
        )
        
        with pytest.raises(requests.HTTPError):
            summarizer.summarize(thread)
