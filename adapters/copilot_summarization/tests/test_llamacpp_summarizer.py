# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

"""Tests for LlamaCppSummarizer."""

from unittest.mock import Mock, patch

import pytest
import requests
from copilot_summarization.factory import create_llm_backend
from copilot_summarization.llamacpp_summarizer import LlamaCppSummarizer
from copilot_summarization.models import Thread


class TestLlamaCppSummarizer:
    """Tests for LlamaCppSummarizer implementation."""

    def test_llamacpp_summarizer_creation(self, llm_driver_config):
        """Test creating a llama.cpp summarizer."""
        summarizer = LlamaCppSummarizer.from_config(llm_driver_config("llamacpp"))
        assert summarizer.model == "mistral"
        assert summarizer.base_url == "http://llama-cpp:8081"
        assert summarizer.timeout == 300

    def test_llamacpp_summarizer_custom_config(self):
        """Test creating a llama.cpp summarizer with custom config."""
        summarizer = LlamaCppSummarizer(
            model="mistral-7b-instruct-v0.2.Q4_K_M", base_url="http://llama-cpp:8080", timeout=60
        )
        assert summarizer.model == "mistral-7b-instruct-v0.2.Q4_K_M"
        assert summarizer.base_url == "http://llama-cpp:8080"
        assert summarizer.timeout == 60

    def test_llamacpp_summarizer_invalid_timeout(self, llm_backend_config):
        """Test invalid timeout is rejected by schema validation."""
        with pytest.raises(ValueError, match="llamacpp_timeout_seconds parameter is invalid"):
            create_llm_backend(llm_backend_config("llamacpp", fields={"llamacpp_timeout_seconds": 0}))

        with pytest.raises(ValueError, match="llamacpp_timeout_seconds parameter is invalid"):
            create_llm_backend(llm_backend_config("llamacpp", fields={"llamacpp_timeout_seconds": -1}))

        with pytest.raises(ValueError, match="llamacpp_timeout_seconds parameter is invalid"):
            create_llm_backend(llm_backend_config("llamacpp", fields={"llamacpp_timeout_seconds": "120"}))  # type: ignore[arg-type]

    @patch("copilot_summarization.llamacpp_summarizer.requests.post")
    def test_llamacpp_summarize_success(self, mock_post, llm_driver_config):
        """Test llama.cpp summarize returns real content from API."""
        # Mock successful API response
        mock_response = Mock()
        mock_response.json.return_value = {
            "content": "This is a summary of the discussion thread generated by llama.cpp."
        }
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response

        summarizer = LlamaCppSummarizer.from_config(
            llm_driver_config(
                "llamacpp",
                fields={
                    "llamacpp_model": "mistral-7b-instruct-v0.2.Q4_K_M",
                    "llamacpp_endpoint": "http://localhost:8080",
                },
            )
        )

        complete_prompt = (
            "Summarize the following discussion thread:\n\nMessage 1:\nMessage 1\n\nMessage 2:\nMessage 2\n\n"
        )
        thread = Thread(thread_id="test-thread-123", messages=["Message 1", "Message 2"], prompt=complete_prompt)

        summary = summarizer.summarize(thread)

        # Verify API was called
        mock_post.assert_called_once()
        call_args = mock_post.call_args
        assert call_args[0][0] == "http://localhost:8080/completion"
        assert call_args[1]["json"]["prompt"]
        assert call_args[1]["json"]["n_predict"] == 512
        assert call_args[1]["json"]["temperature"] == 0.7
        assert "stop" in call_args[1]["json"]

        # Verify complete prompt is used (not built from messages)
        prompt = call_args[1]["json"]["prompt"]
        assert prompt == complete_prompt

        # Verify summary contains real content
        assert summary.thread_id == "test-thread-123"
        assert summary.summary_markdown == "This is a summary of the discussion thread generated by llama.cpp."
        assert summary.llm_backend == "llamacpp"
        assert summary.llm_model == "mistral-7b-instruct-v0.2.Q4_K_M"
        assert summary.tokens_prompt > 0
        assert summary.tokens_completion > 0
        assert summary.latency_ms >= 0

    @patch("copilot_summarization.llamacpp_summarizer.requests.post")
    def test_llamacpp_summarize_empty_response(self, mock_post, llm_driver_config):
        """Test llama.cpp handles empty response gracefully."""
        # Mock empty API response
        mock_response = Mock()
        mock_response.json.return_value = {"content": ""}
        mock_response.raise_for_status = Mock()
        mock_post.return_value = mock_response

        summarizer = LlamaCppSummarizer.from_config(
            llm_driver_config("llamacpp", fields={"llamacpp_model": "mistral-7b-instruct-v0.2.Q4_K_M"})
        )

        thread = Thread(thread_id="test-thread-456", messages=["Message 1"])

        summary = summarizer.summarize(thread)

        # Should return fallback message
        assert "Unable to generate summary" in summary.summary_markdown
        assert summary.thread_id == "test-thread-456"
        assert summary.tokens_completion == 0

    @patch("copilot_summarization.llamacpp_summarizer.requests.post")
    def test_llamacpp_summarize_timeout(self, mock_post, llm_driver_config):
        """Test llama.cpp handles timeout errors."""
        mock_post.side_effect = requests.Timeout("Request timed out")

        summarizer = LlamaCppSummarizer.from_config(
            llm_driver_config("llamacpp", fields={"llamacpp_model": "mistral-7b-instruct-v0.2.Q4_K_M"})
        )

        thread = Thread(thread_id="test-thread-789", messages=["Message 1"])

        with pytest.raises(requests.Timeout):
            summarizer.summarize(thread)

    @patch("copilot_summarization.llamacpp_summarizer.requests.post")
    def test_llamacpp_summarize_connection_error(self, mock_post, llm_driver_config):
        """Test llama.cpp handles connection errors."""
        mock_post.side_effect = requests.ConnectionError("Failed to connect")

        summarizer = LlamaCppSummarizer.from_config(
            llm_driver_config("llamacpp", fields={"llamacpp_model": "mistral-7b-instruct-v0.2.Q4_K_M"})
        )

        thread = Thread(thread_id="test-thread-999", messages=["Message 1"])

        with pytest.raises(requests.ConnectionError):
            summarizer.summarize(thread)

    @patch("copilot_summarization.llamacpp_summarizer.requests.post")
    def test_llamacpp_summarize_http_error(self, mock_post, llm_driver_config):
        """Test llama.cpp handles HTTP errors."""
        mock_response = Mock()
        mock_response.raise_for_status.side_effect = requests.HTTPError("500 Server Error")
        mock_post.return_value = mock_response

        summarizer = LlamaCppSummarizer.from_config(
            llm_driver_config("llamacpp", fields={"llamacpp_model": "mistral-7b-instruct-v0.2.Q4_K_M"})
        )

        thread = Thread(thread_id="test-thread-500", messages=["Message 1"])

        with pytest.raises(requests.HTTPError):
            summarizer.summarize(thread)
