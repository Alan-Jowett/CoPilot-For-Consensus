{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "service_name": "summarization",
  "schema_version": "1.0.0",
  "min_service_version": "0.1.0",
  "metadata": {
    "description": "Summarization service configuration schema",
    "version": "1.0.0"
  },
  "fields": {
    "message_bus_type": {
      "type": "string",
      "source": "env",
      "env_var": "MESSAGE_BUS_TYPE",
      "required": true,
      "description": "Message bus type (rabbitmq, servicebus)"
    },
    "servicebus_use_managed_identity": {
      "type": "bool",
      "source": "env",
      "env_var": "SERVICEBUS_USE_MANAGED_IDENTITY",
      "default": true,
      "required": false,
      "description": "Use Azure managed identity for Service Bus authentication (when MESSAGE_BUS_TYPE=servicebus)"
    },
    "servicebus_fully_qualified_namespace": {
      "type": "string",
      "source": "env",
      "env_var": "SERVICEBUS_FULLY_QUALIFIED_NAMESPACE",
      "required": false,
      "description": "Azure Service Bus fully qualified namespace (when MESSAGE_BUS_TYPE=servicebus, e.g., mybus.servicebus.windows.net)"
    },
    "rabbitmq_host": {
      "type": "string",
      "source": "env",
      "env_var": "RABBITMQ_HOST",
      "default": "messagebus",
      "description": "Message bus hostname"
    },
    "rabbitmq_port": {
      "type": "int",
      "source": "env",
      "env_var": "RABBITMQ_PORT",
      "default": 5672,
      "description": "Message bus port"
    },
    "rabbitmq_username": {
      "type": "string",
      "source": "secret",
      "secret_name": "rabbitmq_username",
      "required": false,
      "description": "RabbitMQ username (secret)"
    },
    "rabbitmq_password": {
      "type": "string",
      "source": "secret",
      "secret_name": "rabbitmq_password",
      "required": false,
      "description": "RabbitMQ password (secret)"
    },
    "doc_store_type": {
      "type": "string",
      "source": "env",
      "env_var": "DOCUMENT_STORE_TYPE",
      "required": true,
      "description": "Document store type (mongodb, cosmosdb)"
    },
    "mongodb_host": {
      "type": "string",
      "source": "env",
      "env_var": "MONGODB_HOST",
      "default": "documentdb",
      "description": "MongoDB hostname (when DOCUMENT_STORE_TYPE=mongodb)"
    },
    "mongodb_port": {
      "type": "int",
      "source": "env",
      "env_var": "MONGODB_PORT",
      "default": 27017,
      "description": "MongoDB port (when DOCUMENT_STORE_TYPE=mongodb)"
    },
    "mongodb_database": {
      "type": "string",
      "source": "env",
      "env_var": "MONGODB_DATABASE",
      "default": "copilot",
      "description": "MongoDB database name (when DOCUMENT_STORE_TYPE=mongodb)"
    },
    "cosmos_endpoint": {
      "type": "string",
      "source": "env",
      "env_var": "COSMOS_ENDPOINT",
      "default": "documentdb",
      "description": "Cosmos DB endpoint URL (when DOCUMENT_STORE_TYPE=cosmosdb)"
    },
    "cosmos_port": {
      "type": "int",
      "source": "env",
      "env_var": "COSMOS_PORT",
      "default": 27017,
      "description": "Cosmos DB port (when DOCUMENT_STORE_TYPE=cosmosdb)"
    },
    "cosmos_database": {
      "type": "string",
      "source": "env",
      "env_var": "COSMOS_DATABASE",
      "default": "copilot",
      "description": "Cosmos DB database name (when DOCUMENT_STORE_TYPE=cosmosdb)"
    },
    "mongodb_username": {
      "type": "string",
      "source": "secret",
      "secret_name": "mongodb_username",
      "required": false,
      "description": "MongoDB username (secret, when DOCUMENT_STORE_TYPE=mongodb)"
    },
    "mongodb_password": {
      "type": "string",
      "source": "secret",
      "secret_name": "mongodb_password",
      "required": false,
      "description": "MongoDB password (secret, when DOCUMENT_STORE_TYPE=mongodb)"
    },
    "vector_store_type": {
      "type": "string",
      "source": "env",
      "env_var": "VECTOR_STORE_TYPE",
      "required": true,
      "description": "Vector store type"
    },
    "vector_store_host": {
      "type": "string",
      "source": "env",
      "env_var": "QDRANT_HOST",
      "default": "vectorstore",
      "description": "Vector store hostname"
    },
    "vector_store_port": {
      "type": "int",
      "source": "env",
      "env_var": "QDRANT_PORT",
      "default": 6333,
      "description": "Vector store port"
    },
    "vector_store_collection": {
      "type": "string",
      "source": "env",
      "env_var": "QDRANT_COLLECTION",
      "default": "message_embeddings",
      "description": "Vector store collection name"
    },
    "SENTENCETRANSFORMERS_DIMENSION": {
      "type": "int",
      "source": "env",
      "env_var": "SENTENCETRANSFORMERS_DIMENSION",
      "default": 384,
      "description": "Embedding vector dimension size"
    },
    "QDRANT_DISTANCE": {
      "type": "string",
      "source": "env",
      "env_var": "QDRANT_DISTANCE",
      "default": "cosine",
      "description": "Distance metric for vector store (cosine, euclidean, dot)"
    },
    "QDRANT_SENTENCETRANSFORMERS_SENTENCETRANSFORMERS_BATCH_SIZE": {
      "type": "int",
      "source": "env",
      "env_var": "QDRANT_SENTENCETRANSFORMERS_SENTENCETRANSFORMERS_BATCH_SIZE",
      "default": 100,
      "description": "Batch size for vector store operations"
    },
    "llm_backend": {
      "type": "string",
      "source": "env",
      "env_var": "LLM_BACKEND",
      "required": true,
      "description": "LLM backend. Valid values: 'local' (Ollama - default, good for NVIDIA GPUs or CPU), 'llamacpp' (llama.cpp - recommended for AMD GPUs), 'openai' (OpenAI API), 'azure' (Azure OpenAI), 'mock' (testing). Choose 'llamacpp' for AMD GPU acceleration with Vulkan/ROCm."
    },
    "llm_model": {
      "type": "string",
      "source": "env",
      "env_var": "LLM_MODEL",
      "default": "mistral",
      "description": "LLM model name"
    },
    "local_llm_endpoint": {
      "type": "string",
      "source": "env",
      "env_var": "OLLAMA_HOST",
      "default": "http://ollama:11434",
      "description": "Local LLM endpoint URL (for local/ollama backend)"
    },
    "llamacpp_endpoint": {
      "type": "string",
      "source": "env",
      "env_var": "LLAMACPP_HOST",
      "default": "http://llama-cpp:8081",
      "description": "llama.cpp server endpoint URL (for llamacpp backend)"
    },
    "llm_temperature": {
      "type": "float",
      "source": "env",
      "env_var": "LLM_TEMPERATURE",
      "default": 0.2,
      "description": "LLM temperature"
    },
    "llm_max_tokens": {
      "type": "int",
      "source": "env",
      "env_var": "LLM_MAX_TOKENS",
      "default": 2048,
      "description": "LLM max tokens"
    },
    "top_k": {
      "type": "int",
      "source": "env",
      "env_var": "TOP_K",
      "default": 12,
      "description": "Number of top chunks to retrieve"
    },
    "citation_count": {
      "type": "int",
      "source": "env",
      "env_var": "CITATION_COUNT",
      "default": 12,
      "description": "Number of citations to include in summaries"
    },
    "llm_timeout_seconds": {
      "type": "int",
      "source": "env",
      "env_var": "LLM_TIMEOUT_SECONDS",
      "default": 300,
      "description": "LLM request timeout in seconds (300s/5min default for CPU inference; increase if using large models on CPU)"
    },
    "max_retries": {
      "type": "int",
      "source": "env",
      "env_var": "RETRY_MAX_ATTEMPTS",
      "default": 3,
      "description": "Maximum retry attempts"
    },
    "retry_delay": {
      "type": "int",
      "source": "env",
      "env_var": "RETRY_DELAY_SECONDS",
      "default": 5,
      "description": "Retry delay in seconds"
    },
    "metrics_type": {
      "type": "string",
      "source": "env",
      "env_var": "METRICS_TYPE",
      "required": true,
      "description": "Metrics backend type (prometheus, pushgateway, azure_monitor, noop)"
    },
    "error_reporter_type": {
      "type": "string",
      "source": "env",
      "env_var": "ERROR_REPORTER_TYPE",
      "required": true,
      "description": "Error reporter type"
    },
    "http_port": {
      "type": "int",
      "source": "env",
      "env_var": "HTTP_PORT",
      "default": 8000,
      "description": "HTTP server port"
    },
    "jwt_auth_enabled": {
      "type": "bool",
      "source": "env",
      "env_var": "JWT_AUTH_ENABLED",
      "default": true,
      "description": "Enable JWT authentication middleware"
    },
    "aisearch_endpoint": {
      "type": "string",
      "source": "env",
      "env_var": "AISEARCH_ENDPOINT",
      "required": false,
      "description": "Azure AI Search service endpoint URL"
    },
    "aisearch_index_name": {
      "type": "string",
      "source": "env",
      "env_var": "AISEARCH_INDEX_NAME",
      "required": false,
      "description": "Azure AI Search index name"
    },
    "aisearch_api_key": {
      "type": "string",
      "source": "env",
      "env_var": "AISEARCH_API_KEY",
      "required": false,
      "description": "Azure AI Search API key (optional if using managed identity)"
    },
    "aisearch_use_managed_identity": {
      "type": "bool",
      "source": "env",
      "env_var": "AISEARCH_USE_MANAGED_IDENTITY",
      "default": true,
      "description": "Use Azure managed identity for AI Search authentication"
    },
    "azure_openai_endpoint": {
      "type": "string",
      "source": "env",
      "env_var": "AZURE_OPENAI_ENDPOINT",
      "required": false,
      "description": "Azure OpenAI service endpoint URL (https://your-resource.openai.azure.com/)"
    },
    "azure_openai_api_key": {
      "type": "string",
      "source": "env",
      "env_var": "AZURE_OPENAI_API_KEY",
      "required": false,
      "description": "Azure OpenAI API key (for LLM backend)"
    },
    "azure_openai_deployment": {
      "type": "string",
      "source": "env",
      "env_var": "AZURE_OPENAI_DEPLOYMENT",
      "required": false,
      "description": "Azure OpenAI deployment name for LLM (e.g., gpt-4o)"
    },
    "azure_openai_api_version": {
      "type": "string",
      "source": "env",
      "env_var": "AZURE_OPENAI_API_VERSION",
      "required": false,
      "description": "Azure OpenAI API version for the LLM backend (e.g., 2024-02-15-preview)"
    }
  }
}
