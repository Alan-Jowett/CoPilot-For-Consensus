# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Copilot-for-Consensus contributors

# Infrastructure Services
# This file contains all infrastructure and monitoring services.
# Included by the top-level docker-compose.yml.

services:
  # Database initialization and validation
  db-init:
    image: mongo:7.0@sha256:8c3ce64d1a433bf57ea79035b48a38ea3e532997a1328fe3266e2e2e8bfb41b6
    depends_on:
      documentdb:
        condition: service_healthy
    environment:
      - MONGO_APP_DB=${MONGO_APP_DB:-copilot}
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    volumes:
      - ./infra/init:/init:ro
      - ./documents/schemas:/schemas:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |-
        set -e
        MONGO_USER="$(cat /run/secrets/mongodb_root_username)"
        MONGO_PASS="$(cat /run/secrets/mongodb_root_password)"
        mongosh "mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin" /init/mongo-init.js
    restart: "no"

  db-validate:
    image: mongo:7.0@sha256:8c3ce64d1a433bf57ea79035b48a38ea3e532997a1328fe3266e2e2e8bfb41b6
    depends_on:
      documentdb:
        condition: service_healthy
      db-init:
        condition: service_completed_successfully
    environment:
      - MONGO_APP_DB=${MONGO_APP_DB:-copilot}
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    volumes:
      - ./infra/test:/test:ro
      - ./documents/schemas:/schemas:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |-
        set -e
        MONGO_USER="$(cat /run/secrets/mongodb_root_username)"
        MONGO_PASS="$(cat /run/secrets/mongodb_root_password)"
        mongosh "mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin" /test/validate-mongo.js
    restart: "no"

  documentdb:
    image: mongo:7.0@sha256:8c3ce64d1a433bf57ea79035b48a38ea3e532997a1328fe3266e2e2e8bfb41b6
    ports:
      - "127.0.0.1:27017:27017"
    volumes:
      - mongo_data:/data/db
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |-
        export MONGO_INITDB_ROOT_USERNAME="$(cat /run/secrets/mongodb_root_username)"
        export MONGO_INITDB_ROOT_PASSWORD="$(cat /run/secrets/mongodb_root_password)"
        exec docker-entrypoint.sh mongod
    healthcheck:
      test: ["CMD", "mongosh", "mongodb://localhost:27017/admin", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest@sha256:70d9599b186ce287be0d2c5ba9a78acb2e86c1a68c9c41449454d0fc3eeb84e8
    # No host port; Grafana is served via API Gateway at /grafana/
    secrets:
      - grafana_admin_user
      - grafana_admin_password
    entrypoint: ["/bin/sh", "-c"]
    environment:
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:8080/grafana/}
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    command:
      - |-
        export GF_SECURITY_ADMIN_USER="$(cat /run/secrets/grafana_admin_user)"
        export GF_SECURITY_ADMIN_PASSWORD="$(cat /run/secrets/grafana_admin_password)"
        export GF_USERS_ALLOW_SIGN_UP=false
        # Serve Grafana from subpath /grafana via API Gateway
        # Ensure trailing slash for subpath to avoid redirect loops
        exec /run.sh
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infra/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./infra/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      monitoring:
        condition: service_healthy
      loki:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 60s
    restart: unless-stopped

  loki:
    image: grafana/loki:latest@sha256:cd6e176883a90c21755f0315688668991634143423f75bdedfef41441b0fdc3c
    ports:
      - "127.0.0.1:3100:3100"
    volumes:
      - ./infra/loki/loki-config.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    # Note: grafana/loki image is distroless and lacks a shell and http tools,
    # so we cannot run an in-container HTTP healthcheck.
    restart: unless-stopped

  messagebus:
    image: rabbitmq:3-management@sha256:e582c0bc7766f3342496d8485efb5a1df782b5ce3886ad017e2eaae442311f69
    ports:
      - "127.0.0.1:5672:5672"
      - "127.0.0.1:15672:15672"
    volumes:
      - ./infra/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json:ro
      - ./infra/rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins:ro
    secrets:
      - rabbitmq_user
      - rabbitmq_pass
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        export RABBITMQ_DEFAULT_USER="$(cat /run/secrets/rabbitmq_user)"
        export RABBITMQ_DEFAULT_PASS="$(cat /run/secrets/rabbitmq_pass)"
        export RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS='-rabbitmq_management load_definitions "/etc/rabbitmq/definitions.json"'
        exec docker-entrypoint.sh rabbitmq-server
    healthcheck:
      # Ensure the node is up AND the AMQP listener is accepting connections
      test: ["CMD-SHELL", "rabbitmq-diagnostics -q ping && rabbitmq-diagnostics -q check_port_connectivity"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  # RabbitMQ Metrics Collection:
  # RabbitMQ 3.8+ includes a built-in Prometheus plugin (rabbitmq_prometheus) that exposes
  # metrics at port 15692. This plugin is enabled via infra/rabbitmq/enabled_plugins.
  # Prometheus scrapes these metrics directly from the messagebus service (messagebus:15692).
  # No separate exporter service is needed.

  mongodb-exporter:
    build:
      context: ./infra/mongodb-exporter
      dockerfile: Dockerfile
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        set -e
        MONGO_USER=$$(cat /run/secrets/mongodb_root_username)
        MONGO_PASS=$$(cat /run/secrets/mongodb_root_password)
        export MONGODB_URI="mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin"
        exec /mongodb_exporter --collect-all
    depends_on:
      documentdb:
        condition: service_healthy
    restart: unless-stopped

  monitoring:
    image: prom/prometheus:latest@sha256:d936808bdea528155c0154a922cd42fd75716b8bb7ba302641350f9f3eaeba09
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest@sha256:6c76395793f40a5e78f120e880ca82c67e39eb908ad90eee8ce755535529d0ec
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ./ollama_models:/root/.ollama
    # Uncomment the deploy section below to enable GPU support (requires NVIDIA GPU + nvidia-container-toolkit)
    # See documents/OLLAMA_GPU_SETUP.md for setup instructions
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/127.0.0.1/11434' "]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  ollama-validate:
    image: curlimages/curl:8.10.1@sha256:d9b4541e214bcd85196d6e92e2753ac6d0ea699f0af5741f8c6cccbfcf00ef4b
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        for i in $$(seq 1 40); do
          code=$$(curl -s -o /dev/null -w "%{http_code}" http://ollama:11434/api/tags || echo 0)
          if [ "$$code" -ge 200 ] && [ "$$code" -lt 500 ]; then echo "Ollama OK ($$code)"; exit 0; fi
          echo "Waiting for Ollama ($$code)"; sleep 3
        done
        echo "Ollama validation failed"; exit 1
    restart: "no"

  ollama-model-loader:
    image: ollama/ollama:latest@sha256:6c76395793f40a5e78f120e880ca82c67e39eb908ad90eee8ce755535529d0ec
    depends_on:
      ollama-validate:
        condition: service_completed_successfully
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./ollama_models:/root/.ollama
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        echo "Pulling mistral model..."
        for i in $$(seq 1 5); do
          ollama pull mistral && echo "âœ“ Model loaded successfully" && exit 0
          echo "Attempt $$i failed, retrying..."; sleep 5
        done
        echo "Failed to pull model after 5 attempts"; exit 1
    restart: "no"

  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "8081:8080"
    volumes:
      - llama_models:/models
    environment:
      # Model configuration - can be overridden
      - LLAMA_ARG_MODEL=${LLAMA_MODEL:-/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf}
      - LLAMA_ARG_CTX_SIZE=${LLAMA_CTX_SIZE:-4096}
      - LLAMA_ARG_N_GPU_LAYERS=${LLAMA_GPU_LAYERS:-35}
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8080
    # AMD GPU Configuration:
    # To enable AMD GPU acceleration, uncomment ONE of the following options based on your setup:
    #
    # Option 1: Vulkan backend (RECOMMENDED for integrated GPUs and Windows)
    # Uncomment these lines for AMD Radeon 780M, 680M, or any AMD GPU on Windows/Linux:
    # devices:
    #   - /dev/dri:/dev/dri
    #
    # Option 2: ROCm backend (for discrete AMD GPUs on Linux only)
    # Uncomment these lines for RX 6000/7000 series on Linux:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: amdgpu
    #           capabilities: [gpu]
    #
    # After uncommenting, also remove the 'llamacpp' profile below to enable the service.
    # See documents/LLAMA_CPP_AMD_SETUP.md for detailed setup instructions.
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 60s
    restart: unless-stopped
    # Note: This service is optional. Only needed when using LLM_BACKEND=llamacpp
    profiles:
      - llamacpp

  # WARNING: The Promtail service mounts the Docker socket (/var/run/docker.sock) with read-only access.
  # This is required for Promtail to discover and label logs from Docker containers.
  # However, even read-only access to the Docker socket can be a security risk, as it allows querying
  # information about all containers on the host. This configuration is intended for development/local
  # use only. For production deployments, REMOVE or restrict this mount and consider alternative log
  # collection strategies. See: https://docs.docker.com/engine/security/#docker-daemon-attack-surface
  promtail:
    image: grafana/promtail:latest@sha256:130b6dd63277d99ce87560c0266c0c30d07bc15ba0a8a590d42215465d4f5363
    volumes:
      - ./infra/promtail/promtail-config.yml:/etc/promtail/config.yml
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      loki:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/127.0.0.1/9080' "]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  pushgateway:
    image: prom/pushgateway:latest@sha256:c835998819105d84d484918908df132807c78c05f3e0e649df0c0479c6780d98
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9091/-/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  mongo-doc-count-exporter:
    image: python:3.11-slim
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        set -e
        MONGO_USER="$(cat /run/secrets/mongodb_root_username)"
        MONGO_PASS="$(cat /run/secrets/mongodb_root_password)"
        export MONGO_URI="mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin"
        export MONGO_DB=copilot
        export PORT=9500
        export SCRAPE_INTERVAL_SEC=5
        pip install -q -r /app/scripts/requirements.txt
        python /app/scripts/mongo_doc_count_exporter.py
    volumes:
      - ./:/app:ro
    depends_on:
      documentdb:
        condition: service_healthy
    restart: unless-stopped

  mongo-collstats-exporter:
    image: python:3.11-slim
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        set -e
        MONGO_USER="$(cat /run/secrets/mongodb_root_username)"
        MONGO_PASS="$(cat /run/secrets/mongodb_root_password)"
        export MONGO_URI="mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin"
        export MONGO_DB=copilot
        export PORT=9503
        export SCRAPE_INTERVAL_SEC=5
        pip install -q -r /app/scripts/requirements.txt
        python /app/scripts/mongo_collstats_exporter.py
    volumes:
      - ./:/app:ro
    depends_on:
      documentdb:
        condition: service_healthy
    restart: unless-stopped

  document-processing-exporter:
    image: python:3.11-slim
    secrets:
      - mongodb_root_username
      - mongodb_root_password
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        set -e
        MONGO_USER="$(cat /run/secrets/mongodb_root_username)"
        MONGO_PASS="$(cat /run/secrets/mongodb_root_password)"
        export MONGO_URI="mongodb://$${MONGO_USER}:$${MONGO_PASS}@documentdb:27017/admin"
        export MONGO_DB=copilot
        export PORT=9502
        export SCRAPE_INTERVAL_SEC=5
        pip install -q -r /app/scripts/requirements.txt
        python /app/scripts/document_processing_exporter.py
    volumes:
      - ./:/app:ro
    depends_on:
      documentdb:
        condition: service_healthy
    restart: unless-stopped

  # WARNING: The cAdvisor service requires access to Docker socket and system directories.
  # This is necessary for collecting container resource metrics but introduces security risks.
  # Only use in development/local environments. For production, use platform-native monitoring.
  # See: https://github.com/google/cadvisor/blob/master/docs/security.md
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1@sha256:3cde6faf0791ebf7b41d6f8ae7145466fed712ea6f252c935294d2608b1af388
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  qdrant-exporter:
    image: python:3.11-slim
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        pip install -q -r /app/scripts/requirements.txt
        python /app/scripts/qdrant_exporter.py
    environment:
      - QDRANT_HOST=vectorstore
      - QDRANT_PORT=6333
      - PORT=9501
      - SCRAPE_INTERVAL_SEC=5
    volumes:
      - ./:/app:ro
    depends_on:
      vectorstore:
        condition: service_healthy
    restart: unless-stopped

  vectorstore:
    image: qdrant/qdrant:latest@sha256:dab6de32f7b2cc599985a7c764db3e8b062f70508fb85ca074aa856f829bf335
    ports:
      - "127.0.0.1:6333:6333"
    volumes:
      - vector_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'echo > /dev/tcp/127.0.0.1/6333' "]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  vectorstore-validate:
    image: curlimages/curl:8.10.1@sha256:d9b4541e214bcd85196d6e92e2753ac6d0ea699f0af5741f8c6cccbfcf00ef4b
    depends_on:
      vectorstore:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |-
        for i in $$(seq 1 40); do
          code=$$(curl -s -o /dev/null -w "%{http_code}" http://vectorstore:6333/collections || echo 0)
          if [ "$$code" -ge 200 ] && [ "$$code" -lt 500 ]; then echo "Qdrant OK ($$code)"; exit 0; fi
          echo "Waiting for Qdrant ($$code)"; sleep 3
        done
        echo "Qdrant validation failed"; exit 1
    restart: "no"

  # API Gateway (NGINX) to unify microservice endpoints under a single port and URI namespace
  # Note: Uses 'service_started' for dependencies to avoid circular dependencies.
  # This means gateway can start before backends are healthy, which may cause
  # 502/503 errors if clients access services before they're ready.
  # The API gateway service provides a unified entry point for user-facing services.
  # Gateway uses 'service_started' dependencies (not 'service_healthy') to prevent circular
  # dependency issues where services might depend on the gateway. This means the gateway can
  # start before backend services are fully ready, potentially causing temporary 502/503 errors.
  # The recommended pattern (used in CI) is to start the gateway AFTER all backend services
  # are healthy. For local development, backend services typically start quickly enough that
  # this is not an issue. If you encounter 502 errors immediately after starting the stack,
  # wait a few seconds for backend services to become healthy.
  # CI workflow starts gateway after all services are healthy (recommended pattern).
  gateway:
    image: nginx:alpine
    ports:
      - "0.0.0.0:8080:8080"
    volumes:
      - ./infra/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      reporting:
        condition: service_healthy
      auth:
        condition: service_healthy
      ingestion:
        condition: service_healthy
      ui:
        condition: service_healthy
      grafana:
        condition: service_healthy
    healthcheck:
      # Avoid relying on external HTTP tools (wget/curl) which are not in nginx:alpine by default.
      # Check that the nginx master process is running via its pid file.
      test: ["CMD-SHELL", "[ -f /var/run/nginx.pid ] && kill -0 $(cat /var/run/nginx.pid) || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s
    restart: unless-stopped
